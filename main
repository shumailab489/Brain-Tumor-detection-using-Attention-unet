#Setup ENV
import os
import cv2
import glob
import PIL
import shutil
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from skimage import data
from skimage.util import montage 
import skimage.transform as skTrans
from skimage.transform import rotate
from skimage.transform import resize
from PIL import Image, ImageOps  


# neural imaging
import nilearn as nl
import nibabel as nib
import nilearn.plotting as nlplt
!pip install git+https://github.com/miykael/gif_your_nifti # nifti to gif 
import gif_your_nifti.core as gif2nif


# ml libs
import keras
import keras.backend as K
from keras.callbacks import CSVLogger
import tensorflow as tf
from tensorflow.keras.utils import plot_model
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import *
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard
from tensorflow.keras.layers.experimental import preprocessing
np.set_printoptions(precision=3, suppress=True)

!pip3 install -U segmentation-models
%env SM_FRAMEWORK=tf.keras
import segmentation_models as sm
import tensorflow as tf
tf.keras.backend.set_image_data_format('channels_last')


import sys
import os
import glob
import random
import time

import numpy as np
import pandas as pd

import cv2
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import ImageGrid
from sklearn.model_selection import train_test_split
from segmentation_models import Unet, Linknet, PSPNet, FPN
import keras
from segmentation_models.utils import set_trainable
from torch.utils.data import Dataset
from keras.models import load_model
from tensorflow.keras import utils as np_utils
# DEFINE seg-areas  
SEGMENT_CLASSES = {
    0 : 'Not Tumor',
    1 : 'Necrotic/ Core', # or NON-ENHANCING tumor CORE
    2 : 'Edema',
    3 : 'Enhancing' # original 4 -> converted into 3 later
}

# there are 155 slices per volume
# to start at 5 and use 145 slices means we will skip the first 5 and last 5 
VOLUME_SLICES = 100
VOLUME_START_AT = 22 # first slice of volume that we will include
TRAIN_DATASET_PATH = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/'
VALIDATION_DATASET_PATH =  '../input/brats20-dataset-training-validation/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData'

test_image_flair=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_002/BraTS20_Training_002_flair.nii').get_fdata()
test_image_t1=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_002/BraTS20_Training_002_t1.nii').get_fdata()
test_image_t1ce=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_002/BraTS20_Training_002_t1ce.nii').get_fdata()
test_image_t2=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_002/BraTS20_Training_002_t2.nii').get_fdata()
test_mask=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_002/BraTS20_Training_002_seg.nii').get_fdata()


fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5, figsize = (20, 10))
slice_w = 25
ax1.imshow(test_image_flair[:,:,test_image_flair.shape[0]//2-slice_w], cmap = 'gray')
ax1.set_title('Image TFlair')
ax2.imshow(test_image_t1[:,:,test_image_t1.shape[0]//2-slice_w], cmap = 'gray')
ax2.set_title('Image T1')
ax3.imshow(test_image_t1ce[:,:,test_image_t1ce.shape[0]//2-slice_w], cmap = 'gray')
ax3.set_title('Image T1CE')
ax4.imshow(test_image_t2[:,:,test_image_t2.shape[0]//2-slice_w], cmap = 'gray')
ax4.set_title('Image T2')
ax5.imshow(test_mask[:,:,test_mask.shape[0]//2-slice_w])
ax5.set_title('Mask')
# Skip 50:-50 slices since there is not much to see
fig, ax1 = plt.subplots(1, 1, figsize = (15,15))
ax1.imshow(rotate(montage(test_image_t1[50:-50,:,:]), 90, resize=True), cmap ='gray')
# Skip 50:-50 slices since there is not much to see
fig, ax1 = plt.subplots(1, 1, figsize = (15,15))
ax1.imshow(rotate(montage(test_mask[60:-60,:,:]), 90, resize=True), cmap ='gray')
shutil.copy2(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_flair.nii', './test_gif_BraTS20_Training_001_flair.nii')
gif2nif.write_gif_normal('./test_gif_BraTS20_Training_001_flair.nii')
niimg = nl.image.load_img(TRAIN_DATASET_PATH + 'BraTS20_Training_013/BraTS20_Training_013_flair.nii')
nimask = nl.image.load_img(TRAIN_DATASET_PATH + 'BraTS20_Training_013/BraTS20_Training_013_seg.nii')

fig, axes = plt.subplots(nrows=4, figsize=(30, 40))


nlplt.plot_anat(niimg,
                title='BraTS20_Training_013_flair.nii plot_anat',
                axes=axes[0])

nlplt.plot_epi(niimg,
               title='BraTS20_Training_013_flair.nii plot_epi',
               axes=axes[1])

nlplt.plot_img(niimg,
               title='BraTS20_Training_013_flair.nii plot_img',
               axes=axes[2])

nlplt.plot_roi(nimask, 
               title='BraTS20_Training_013_flair.nii with mask plot_roi',
               bg_img=niimg, 
               axes=axes[3], cmap='Paired')

plt.show()
# dice loss as defined above for 4 classes
def dice_coef(y_true, y_pred, smooth=1.0):
    class_num = 4
    for i in range(class_num):
        y_true_f = K.flatten(y_true[:,:,:,i])
        y_pred_f = K.flatten(y_pred[:,:,:,i])
        intersection = K.sum(y_true_f * y_pred_f)
        loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))
   #     K.print_tensor(loss, message='loss value for class {} : '.format(SEGMENT_CLASSES[i]))
        if i == 0:
            total_loss = loss
        else:
            total_loss = total_loss + loss
    total_loss = total_loss / class_num
#    K.print_tensor(total_loss, message=' total dice coef: ')
    return total_loss


 
# define per class evaluation of dice coef
# inspired by https://github.com/keras-team/keras/issues/9395
def dice_coef_necrotic(y_true, y_pred, epsilon=1e-6):
    intersection = K.sum(K.abs(y_true[:,:,:,1] * y_pred[:,:,:,1]))
    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,1])) + K.sum(K.square(y_pred[:,:,:,1])) + epsilon)

def dice_coef_edema(y_true, y_pred, epsilon=1e-6):
    intersection = K.sum(K.abs(y_true[:,:,:,2] * y_pred[:,:,:,2]))
    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,2])) + K.sum(K.square(y_pred[:,:,:,2])) + epsilon)

def dice_coef_enhancing(y_true, y_pred, epsilon=1e-6):
    intersection = K.sum(K.abs(y_true[:,:,:,3] * y_pred[:,:,:,3]))
    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,3])) + K.sum(K.square(y_pred[:,:,:,3])) + epsilon)



# Computing Precision 
def precision(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision

    
# Computing Sensitivity      
def sensitivity(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    return true_positives / (possible_positives + K.epsilon())


# Computing Specificity
def specificity(y_true, y_pred):
    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))
    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))
    return true_negatives / (possible_negatives + K.epsilon())
# lists of directories with studies
train_and_val_directories = [f.path for f in os.scandir(TRAIN_DATASET_PATH) if f.is_dir()]

# file BraTS20_Training_355 has ill formatted name for for seg.nii file
train_and_val_directories.remove(TRAIN_DATASET_PATH+'BraTS20_Training_355')


def pathListIntoIds(dirList):
    x = []
    for i in range(0,len(dirList)):
        x.append(dirList[i][dirList[i].rfind('/')+1:])
    return x

train_and_test_ids = pathListIntoIds(train_and_val_directories); 

    
train_test_ids, val_ids = train_test_split(train_and_test_ids,test_size=0.2) 
train_ids, test_ids = train_test_split(train_test_ids,test_size=0.15) 
IMG_SIZE = 128
class DataGenerator(keras.utils.Sequence):
    'Generates data for Keras'
    def __init__(self, list_IDs, dim=(IMG_SIZE,IMG_SIZE), batch_size = 1, n_channels = 2, shuffle=True):
        'Initialization'
        self.dim = dim
        self.batch_size = batch_size
        self.list_IDs = list_IDs
        self.n_channels = n_channels
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.list_IDs) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]

        # Find list of IDs
        Batch_ids = [self.list_IDs[k] for k in indexes]

        # Generate data
        X, y = self.__data_generation(Batch_ids)

        return X, y

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, Batch_ids):
        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)
        # Initialization
        X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))
        y = np.zeros((self.batch_size*VOLUME_SLICES, 240, 240))
        Y = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, 4))

        
        # Generate data
        for c, i in enumerate(Batch_ids):
            case_path = os.path.join(TRAIN_DATASET_PATH, i)

            data_path = os.path.join(case_path, f'{i}_flair.nii');
            flair = nib.load(data_path).get_fdata()    

            data_path = os.path.join(case_path, f'{i}_t1ce.nii');
            ce = nib.load(data_path).get_fdata()
            
            data_path = os.path.join(case_path, f'{i}_seg.nii');
            seg = nib.load(data_path).get_fdata()
  

        
            for j in range(VOLUME_SLICES):
                 X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));
                 X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));

                 y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT];
                    
        # Generate masks
        y[y==4] = 3;
        mask = tf.one_hot(y, 4);
        Y = tf.image.resize(mask, (IMG_SIZE, IMG_SIZE));
        return X/np.max(X), Y
        
training_generator = DataGenerator(train_ids)
valid_generator = DataGenerator(val_ids)
test_generator = DataGenerator(test_ids)
# show number of data for each dir 
def showDataLayout():
    plt.bar(["Train","Valid","Test"],
    [len(train_ids), len(val_ids), len(test_ids)], align='center',color=[ 'green','red', 'blue'])
    plt.legend()

    plt.ylabel('Number of images')
    plt.title('Data distribution')

    plt.show()
    
showDataLayout()
niimg = nl.image.load_img(TRAIN_DATASET_PATH + train_ids[0] + '/' + train_ids[0] + '_flair.nii')
nimask = nl.image.load_img(TRAIN_DATASET_PATH + train_ids[0] + '/' + train_ids[0] + '_seg.nii')
​
fig, axes = plt.subplots(nrows=4, figsize=(30, 40))
​
​
nlplt.plot_anat(niimg,
                title= train_ids[0] + '/' + train_ids[0] + '_flair.nii' + 'plot_anat',
                axes=axes[0])
​
nlplt.plot_epi(niimg,
               title= train_ids[0] + '/' + train_ids[0] + '_flair.nii' + 'plot_epi',
               axes=axes[1])
​
nlplt.plot_img(niimg,
               title= train_ids[0] + '/' + train_ids[0] + '_flair.nii' + 'plot_img',
               axes=axes[2])
​
nlplt.plot_roi(nimask, 
               title= train_ids[0] + '/' + train_ids[0] + '_flair.nii' +'mask plot_roi',
               bg_img=niimg, 
               axes=axes[3], cmap='Paired')
​
plt.show()
csv_logger = CSVLogger('training.log', separator=',', append=False)


callbacks = [
#     keras.callbacks.EarlyStopping(monitor='loss', min_delta=0,
#                               patience=2, verbose=1, mode='auto'),
      keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=2, min_lr=0.000001, verbose=1),
#  keras.callbacks.ModelCheckpoint(filepath = 'model_.{epoch:02d}-{val_loss:.6f}.m5',
#                             verbose=1, save_best_only=True, save_weights_only = True)
        csv_logger
    ]
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, BatchNormalization, Activation
from tensorflow.keras.models import Model

def attention_gate(input_1, input_2, filters):
    g1 = Conv2D(filters, 1, strides=1, padding='same')(input_1)
    x1 = Conv2D(filters, 1, strides=1, padding='same')(input_2)
    x1 = BatchNormalization()(x1)
    x1 = Activation('relu')(x1)
    x1 = Concatenate(axis=-1)([g1, x1])
    x1 = Conv2D(1, 1, activation='sigmoid', padding='same')(x1)
    return x1

def conv_block(input_tensor, filters, kernel_size=3):
    x = Conv2D(filters, kernel_size, padding='same')(input_tensor)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    return x

def UNet_Attention(input_shape, num_classes=4):
    inputs = Input(shape=input_shape)

    # Encoder
    conv1 = conv_block(inputs, 64)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
    conv2 = conv_block(pool1, 128)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
    conv3 = conv_block(pool2, 256)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
    conv4 = conv_block(pool3, 512)
    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)

    # Center
    center = conv_block(pool4, 1024)

    # Decoder
    up4 = UpSampling2D(size=(2, 2))(center)
    up4 = Conv2D(512, 2, activation='relu', padding='same')(up4)
    att4 = attention_gate(conv4, up4, 512)
    merge4 = Concatenate()([conv4, att4])
    up_conv4 = conv_block(merge4, 512)

    up3 = UpSampling2D(size=(2, 2))(up_conv4)
    up3 = Conv2D(256, 2, activation='relu', padding='same')(up3)
    att3 = attention_gate(conv3, up3, 256)
    merge3 = Concatenate()([conv3, att3])
    up_conv3 = conv_block(merge3, 256)

    up2 = UpSampling2D(size=(2, 2))(up_conv3)
    up2 = Conv2D(128, 2, activation='relu', padding='same')(up2)
    att2 = attention_gate(conv2, up2, 128)
    merge2 = Concatenate()([conv2, att2])
    up_conv2 = conv_block(merge2, 128)

    up1 = UpSampling2D(size=(2, 2))(up_conv2)
    up1 = Conv2D(64, 2, activation='relu', padding='same')(up1)
    att1 = attention_gate(conv1, up1, 64)
    merge1 = Concatenate()([conv1, att1])
    up_conv1 = conv_block(merge1, 64)

    # Output
    outputs = Conv2D(num_classes, 1, activation='softmax')(up_conv1)

    model = Model(inputs=[inputs], outputs=[outputs])
    return model

# Define input shape (128x128x2 for example)
input_shape = (128, 128, 2)

# Create the model
model = UNet_Attention(input_shape)
 
# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy' , dice_coef])

# Print model summary
model.summary()
import tensorflow.keras.backend as K

# Clear previous session
K.clear_session()

# Train the Attention UNet model
history = model.fit(training_generator,
                    epochs=5,
                    steps_per_epoch=len(train_ids),
                    callbacks=callbacks,
                    validation_data=valid_generator)

# Save the trained model
model.save("attention_unet_model.h5")
# finally, save the model to Huggingface
# Install the huggingface_hub library
!pip install huggingface_hub

# Import the notebook_login function
from huggingface_hub import notebook_login

# Log in to Hugging Face
notebook_login()
from huggingface_hub import HfApi

api = HfApi()
repo_id = "shumailabatool/brain_tumor_detection"
try:
    api.create_repo(repo_id=repo_id)
    print(f"Repo {repo_id} created")
except Exception as e:
    print(f"Repo {repo_id} already exists or an error occurred: {e}")
# Upload the model file
api.upload_file(
    path_or_fileobj="attention_unet_model.h5",
    path_in_repo="attention_unet_model.h5",
    repo_id=repo_id,
    repo_type="model"
)
import json
import shutil
from huggingface_hub import notebook_login, HfApi
from IPython.display import FileLink

# Configuration for the model
config = {
    "architecture": "attention_unet_model.h5",
    "input_shape": [128, 128, 2],
    "num_classes": 4,
    "optimizer": "adam",
    "loss": "categorical_crossentropy",
    "metrics": ["accuracy"]
}

# Tokenizer configuration (not needed for this model, but included as an example)
tokenizer_config = {
    "do_lower_case": False
}

# Model card information
model_card = {
    "model_name": "attention_unet_model.h5",
    "description": "This model performs brain tumor segmentation using an Attention UNet architecture.",
    "tasks": ["segmentation"],
    "license": "apache-2.0",
    "dataset": "BraTS2020",
    "model_architecture": "attention_unet_model.h5"
}

# Save the configuration as a JSON file
with open('config.json', 'w') as config_file:
    json.dump(config, config_file, indent=4)

# Save the tokenizer configuration as a JSON file
with open('tokenizer_config.json', 'w') as tokenizer_config_file:
    json.dump(tokenizer_config, tokenizer_config_file, indent=4)

# Save the model card information as a JSON file
with open('model_card.json', 'w') as model_card_file:
    json.dump(model_card, model_card_file, indent=4)

# Provide download links for each file
FileLink(r'config.json'), FileLink(r'tokenizer_config.json'), FileLink(r'model_card.json')

# Log in to Hugging Face
notebook_login()

# Initialize the API
api = HfApi()
repo_id = "shumailabatool/brain_tumor_detection"

try:
    api.create_repo(repo_id=repo_id)
    print(f"Repo {repo_id} created")
except Exception as e:
    print(f"Repo {repo_id} already exists or an error occurred: {e}")

# Upload the config.json file
api.upload_file(
    path_or_fileobj="config.json",
    path_in_repo="config.json",
    repo_id=repo_id,
    repo_type="model"
)
print("Uploaded config.json")

# Upload the tokenizer_config.json file
api.upload_file(
    path_or_fileobj="tokenizer_config.json",
    path_in_repo="tokenizer_config.json",
    repo_id=repo_id,
    repo_type="model"
)
print("Uploaded tokenizer_config.json")

# Upload the model_card.json file
api.upload_file(
    path_or_fileobj="model_card.json",
    path_in_repo="model_card.json",
    repo_id=repo_id,
    repo_type="model"
)
print("Uploaded model_card.json")
